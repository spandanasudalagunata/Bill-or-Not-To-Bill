---
title: "Data Preparation & Cleaning"
output: github_document
---

```{r setup, include=F}
knitr::opts_chunk$set(
  fig.path = "markdown_figs/data_preparation-"
)
```

```{r, message=FALSE}
library(readxl)
library(dplyr)
library(magrittr)
library(tm)
library(keras)
library(SnowballC)
```

## 1. Loading The Data

We will only load the first 100 observations

```{r, warning=FALSE, message=FALSE}
# Encode specific kinds of values as NA while reading excel
non_bill_df <- read_excel("data/december_non-bill_calls.xlsx", na = c("", "---"), n_max = 1000)
billed_df <- read_excel("data/december_billed_calls.xlsx", na = c("", "---"), n_max = 1000)
```

We will combine `non_bill_df` and `billed_df` into a dataframe called `billing_df`.

```{r}
billing_df <- bind_rows(non_bill_df, billed_df)
str(billing_df, nchar.max = 20, vec.len = 3)
```

Based on initial discussions and research into the meaning of some of the features in this dataset, we have categorized the following features as being not __important__.

```{r, echo=FALSE}
features_to_rm <- c(
  "SR Address Line 1", 
  "SR City",
  "SR Status",
  "Activity Status",
  "Charges Status",
  "SR Coverage Hours..11",
  "SR Coverage Hours..28",
  "Br Region Desc",
  "Activity Facts Call Num"
)
features_to_rm
```

> The features `SR Coverage Hours...11` and `SR Coverage Hours...28` were created by R because the excel contained two columns with the name `SR Coverage Hours`.
The features have been stored in variable called `features_to_rm`, the next step is to remove these `r length(features_to_rm)` features from the `billing_df` dataset. This step reduces our number of features from __29 to 20__.

```{r}
billing_df <- billing_df %>% select(-features_to_rm)
str(billing_df, nchar.max = 20, vec.len = 3)
```

## 2. Cleaning The Data

### 2.1 Encoding The Variables

We can notice that R has miss categorized some of the features in our dataset. There are certain features that are supposed to be read as categorical such as:

```{r echo=FALSE}
char_to_factors <- c(
  "Invoiced (Y/N)",
  "Activity Type",
  "Activity Trouble Code",
  "Coverage Type",
  "Base Call YN",
  "SR Type",
  "SR Device",
  "SR Site",
  "SR Owner (Q#)",
  "SR Serial Number",
  "Cash Vendor & Consumable Contracts"
)
char_to_factors
```

Lets encode the features in `char_to_factors` as factors

```{r}
billing_df <- billing_df %>% mutate_at(char_to_factors, factor)
billing_df %>% summary()
```

### 2.2 Free Form Text

The features in our dataset that are free form text are the features `Billing Notes` and `Call Text`.

Below is a preview of `Call Text`

```{r}
billing_df$`Call Text` %>% head(3)
```

Below is a preview of `Billing Notes`

```{r}
billing_df$`Billing Notes` %>% extract(c(3, 5, 1))
```

```{r}
call_text <-  use_series(billing_df, `Call Text`)
billing_notes <-  use_series(billing_df, `Billing Notes`)
item_desc <- use_series(billing_df, `Item Desc`)
```

```{r}
call_text_corpus <- VCorpus(VectorSource(call_text), readerControl = list(language = "en"))
bill_notes_corpus <- VCorpus(VectorSource(billing_notes), readerControl = list(language = "en"))
item_desc_corpus <- VCorpus(VectorSource(item_desc), readerControl = list(language = "en"))
```

```{r}
call_text_corpus %>% extract(1:3) %>% inspect()
```

```{r}
call_text_corpus %>% head(3) %>% lapply(function (doc) doc$content)
```

To clean our data set we will have to: 

* Convert the text to lower case, so that words like "write" and "Write" are considered the same word
* Remove numbers
* Remove English stopwords e.g "the", "is", "of", etc.
* Remove punctuation e.g ",", "?", etc.
* Eliminate extra white spaces
* Stemming our text 

Using the `tm` package we will apply transformations to each text document in the `call_text_corpus` to clean the text document.

```{r}
replace_asterix <- function(document) {
  gsub(pattern = "\\*", replacement = " ", document)
}
add_space_period <- function(document) {
  gsub(pattern = "\\.", replacement = ". ", document)
}
remove_single_chars <- function(document) {
  gsub(pattern = "\\s[a-z]\\s", replacement = " ", document)
}
clean_up <- function(corpus) {
  corpus %>% 
    # Convert the text to lower case
    tm_map(content_transformer(tolower)) %>%
    # Replace asterics "*" with an empty space
    tm_map(content_transformer(replace_asterix)) %>%
    # Add a space after a period
    tm_map(content_transformer(add_space_period)) %>%
    # Remove numbers
    tm_map(removeNumbers) %>%
    # Remove english common stopwords
    tm_map(removeWords, stopwords("english")) %>%
    # Remove words related to time
    tm_map(removeWords, c("pm", "am", "edt")) %>%
    # Remove punctuations
    tm_map(removePunctuation) %>%
    # Remove orphaned letters
    tm_map(content_transformer(remove_single_chars)) %>%
    # Eliminate extra white spaces
    tm_map(stripWhitespace) %>%
    # strip trailing and leading whitespace
    tm_map(content_transformer(trimws)) %>%
    # Stem words
    tm_map(stemDocument)
}
call_text_cleaned <- clean_up(call_text_corpus)
bill_notes_cleaned <- clean_up(bill_notes_corpus)
item_desc_cleaned <- clean_up(item_desc_corpus)
```


```{r}
call_text_cleaned  %>% lapply(function (doc) doc$content) %>% extract(1:5)
```

```{r}
bill_notes_cleaned %>% lapply(function (doc) doc$content) %>% extract(1:5)
```

```{r}
billing_df$`Call Text` <- call_text_cleaned %>% sapply(function (doc) doc$content)
billing_df$`Billing Notes` <- bill_notes_cleaned %>% sapply(function (doc) doc$content)
billing_df$`Item Desc` <- item_desc_cleaned %>% sapply (function (doc) doc$content)
```


## 3. Tokenization

```{r}
CONSTANTS <- list(
  # We will only consider the top 10,000 words in the dataset
  MAX_WORDS = 10000,
  # We will cut text after 100 words
  MAX_LEN = 100
)
```

### 3.1 Tokeninzing Categorical data

We will start off by encoding the labels of `Invoiced (Y/N)` using the `to_categorical` from keras

```{r}
labels <- billing_df %>%
  use_series("Invoiced (Y/N)") %>%
  as.numeric() %>%
  subtract(1) %>%
  ifelse(("Invoiced (Y/N)" == "Y"),1) %>%
  as.array()
cat('Shape of label tensor:', dim(labels), "\n")
View(labels)
```


```{r}
type <- billing_df %>%
  use_series("SR Type") %>%
  as.numeric() %>%
  to_categorical() %>%
  as.array()
cat('Shape of SR Type tensor:', dim(type), "\n")
```

```{r}
View(type)
```


### 3.2 Tokenizing Free Form Text

We will tokenize each free form text: `Call Text`, `Billing Notes`, and `Item Description` separately. 

## 3.2.1 Call Text

We will start out by tokenizing `Call Text`:

```{r}
call_text_df <- billing_df %>% select(c("Call Text"))
```

A `tokenizer` object will be created and configured to only take into account the 20,000 most common words, then builds the word index.

```{r}
tokenizer <- text_tokenizer(num_words = CONSTANTS$MAX_WORDS) %>% 
  fit_text_tokenizer(call_text_df$`Call Text`)
```

We then turn the texts into lists of integer indices

```{r}
sequences <- texts_to_sequences(tokenizer, call_text_df$`Call Text`)
```

How you can recover the word index that was computed

```{r}
word_index <- tokenizer$word_index
cat("Found", length(word_index), "unique tokens.\n")
```

```{r}
call_text_data <- pad_sequences(sequences, maxlen = CONSTANTS$MAX_LEN)
cat("Shape of data tensor:", dim(call_text_data), "\n")
```

## 3.2.2 Billing Notes

We then tokenize `Billing Notes`:

```{r}
billing_notes_df <- billing_df %>% select("Billing Notes")
```

```{r}
tokenizer <- text_tokenizer(num_words = CONSTANTS$MAX_WORDS) %>% 
  fit_text_tokenizer(billing_notes_df$`Billing Notes`)
```

```{r}
sequences <- texts_to_sequences(tokenizer, billing_notes_df$`Billing Notes`)
```

```{r}
word_index <- tokenizer$word_index
cat("Found", length(word_index), "unique tokens.\n")
```

```{r}
billing_notes_data <- pad_sequences(sequences, maxlen = CONSTANTS$MAX_LEN)
cat("Shape of data tensor:", dim(billing_notes_data), "\n")
```


## 3.2.3 Item Description

Finally we tokenize `Item Desc`:

```{r}
item_desc_df <- billing_df %>% select(c("Item Desc"))
```

```{r}
tokenizer <- text_tokenizer(num_words = CONSTANTS$MAX_WORDS) %>% 
  fit_text_tokenizer(item_desc_df$`Item Desc`)
```

```{r}
sequences <- texts_to_sequences(tokenizer, item_desc_df$`Item Desc`)
```

```{r}
word_index <- tokenizer$word_index
cat("Found", length(word_index), "unique tokens.\n")
```

```{r}
item_desc_data <- pad_sequences(sequences, maxlen = CONSTANTS$MAX_LEN)
cat("Shape of data tensor:", dim(item_desc_data), "\n")
```


```{r}
save.image(file = "data/data_prep_workspace.RData")
save(billing_df, CONSTANTS, call_text_data, billing_notes_data, item_desc_data, type, file="data/data_preparation.RData")
```
